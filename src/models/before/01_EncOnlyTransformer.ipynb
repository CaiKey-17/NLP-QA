{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StVLPWhnj5pM",
        "outputId": "cb28397d-73db-4993-fdd5-91af473e2443"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install evaluate -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYiJ5yhLkuMM",
        "outputId": "c8c976a2-5910-4735-dc73-e40a927768c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h6Em52Igl-Ja"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "import json, os, numpy as np\n",
        "import re, random\n",
        "from collections import Counter\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "vLK0pEcxSvqx"
      },
      "outputs": [],
      "source": [
        "class QAProcessorPhoBERT:\n",
        "    def __init__(self, tokenizer, max_length=256):\n",
        "        self.tok = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def encode_example(self, context, question, answer):\n",
        "        # 1. exact char span\n",
        "        start_char = context.find(answer)\n",
        "        if start_char == -1:\n",
        "            return None\n",
        "        end_char = start_char + len(answer)\n",
        "\n",
        "        # 2. tokenize context but KEEP char alignment using regex split\n",
        "        #    (PhoBERT's tokenize() breaks spacing, so we split manually into words)\n",
        "        import re\n",
        "        words = re.findall(r\"\\S+|\\s+\", context)\n",
        "\n",
        "        tokens = []\n",
        "        offsets = []\n",
        "        char_idx = 0\n",
        "\n",
        "        for w in words:\n",
        "            if w.isspace():\n",
        "                char_idx += len(w)\n",
        "                continue\n",
        "\n",
        "            sub_toks = self.tok.tokenize(w)\n",
        "            for st in sub_toks:\n",
        "                clean = st.lstrip(\"‚ñÅ\")\n",
        "                s = context.find(clean, char_idx)\n",
        "                if s == -1:\n",
        "                    # fallback to sequential char index\n",
        "                    s = char_idx\n",
        "                e = s + len(clean)\n",
        "                tokens.append(st)\n",
        "                offsets.append((s, e))\n",
        "            char_idx = context.find(w, char_idx) + len(w)\n",
        "\n",
        "        # map char span ‚Üí token span\n",
        "        start_tok = end_tok = None\n",
        "        for i, (s, e) in enumerate(offsets):\n",
        "            if s <= start_char < e:\n",
        "                start_tok = i\n",
        "            if s < end_char <= e:\n",
        "                end_tok = i\n",
        "\n",
        "        if start_tok is None or end_tok is None:\n",
        "            return None\n",
        "\n",
        "        # encode pair\n",
        "        enc = self.tok(\n",
        "            question,\n",
        "            context,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=None,\n",
        "            return_overflowing_tokens=False\n",
        "        )\n",
        "\n",
        "        enc.pop(\"token_type_ids\", None)\n",
        "\n",
        "        # question token count\n",
        "        q_tokens = self.tok.tokenize(question)\n",
        "\n",
        "        offset = 1 + len(q_tokens) + 1  # <s> Q </s>\n",
        "\n",
        "        start_pos = start_tok + offset\n",
        "        end_pos = end_tok + offset\n",
        "\n",
        "        if end_pos >= self.max_length:\n",
        "            return None\n",
        "\n",
        "        enc[\"start_positions\"] = start_pos\n",
        "        enc[\"end_positions\"] = end_pos\n",
        "        return enc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "_UgiJSyZSwll"
      },
      "outputs": [],
      "source": [
        "class QADataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=256):\n",
        "        self.processor = QAProcessorPhoBERT(tokenizer, max_length)\n",
        "        self.features = []\n",
        "\n",
        "        for ex in data:\n",
        "            item = self.processor.encode_example(\n",
        "                ex[\"context\"], ex[\"question\"], ex[\"answer\"]\n",
        "            )\n",
        "            if item is not None:\n",
        "                self.features.append(item)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feat = self.features[idx]\n",
        "        return {k: torch.tensor(v) for k, v in feat.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTUvaH4ZbxT7",
        "outputId": "5062c3a6-7589-4cbb-e0d4-05b19a2c22ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üëâ Original: 25061\n",
            "üëâ Cleaned: 24649\n",
            "üëâ Dropped: 412\n",
            "üî• Keep ratio: 0.9836\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def normalize_text(s):\n",
        "    \"\"\"Chu·∫©n h√≥a ƒë·ªÉ lo·∫°i l·ªói kho·∫£ng tr·∫Øng, unicode, xu·ªëng d√≤ng.\"\"\"\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "\n",
        "    s = s.replace(\"‚Äì\", \"-\")      # normalize dash\n",
        "    s = s.replace(\"‚Äî\", \"-\")\n",
        "    s = s.replace(\"‚Äú\", \"\\\"\").replace(\"‚Äù\", \"\\\"\")\n",
        "    s = s.replace(\"‚Äô\", \"'\")\n",
        "    s = s.replace(\"‚Ä¶\", \"...\")\n",
        "\n",
        "    # b·ªè kho·∫£ng tr·∫Øng d∆∞\n",
        "    s = re.sub(r\"\\s+\", \" \", s.strip())\n",
        "\n",
        "    return s\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "def extract_best_span(context, answer):\n",
        "    \"\"\"N·∫øu answer kh√¥ng kh·ªõp EXACT, t√¨m ƒëo·∫°n t∆∞∆°ng t·ª± nh·∫•t trong context.\"\"\"\n",
        "    ctx = normalize_text(context)\n",
        "    ans = normalize_text(answer)\n",
        "\n",
        "    # 1) exact match\n",
        "    if ans in ctx:\n",
        "        return ans\n",
        "\n",
        "    # 2) approx match (fuzzy)\n",
        "    match = SequenceMatcher(None, ctx, ans).find_longest_match(\n",
        "        0, len(ctx),\n",
        "        0, len(ans)\n",
        "    )\n",
        "\n",
        "    span = ctx[match.a : match.a + match.size]\n",
        "\n",
        "    # span h·ª£p l·ªá ph·∫£i c√≥ √≠t nh·∫•t 2 t·ª´\n",
        "    if len(span.split()) >= 2:\n",
        "        return span\n",
        "\n",
        "    return None\n",
        "import json\n",
        "\n",
        "def clean_dataset_for_phobert(raw_data):\n",
        "    cleaned = []\n",
        "    dropped = []\n",
        "\n",
        "    for ex in raw_data:\n",
        "        ctx = normalize_text(ex[\"context\"])\n",
        "        ques = normalize_text(ex[\"question\"])\n",
        "        ans = normalize_text(ex[\"answer\"])\n",
        "\n",
        "        span = extract_best_span(ctx, ans)\n",
        "\n",
        "        if span is None:\n",
        "            dropped.append(ex)\n",
        "        else:\n",
        "            cleaned.append({\n",
        "                \"context\": ctx,\n",
        "                \"question\": ques,\n",
        "                \"answer\": span\n",
        "            })\n",
        "\n",
        "    return cleaned, dropped\n",
        "# Load file g·ªëc\n",
        "with open(\"/content/drive/MyDrive/NLP/qa_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "# CLEAN\n",
        "cleaned_data, dropped_data = clean_dataset_for_phobert(raw_data)\n",
        "\n",
        "print(\"üëâ Original:\", len(raw_data))\n",
        "print(\"üëâ Cleaned:\", len(cleaned_data))\n",
        "print(\"üëâ Dropped:\", len(dropped_data))\n",
        "print(f\"üî• Keep ratio: {len(cleaned_data)/len(raw_data):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXHzQxWdS2KB",
        "outputId": "965a8168-9aec-4e91-9231-f6759acd4c97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 19719\n",
            "Val: 2465\n",
            "Test: 2465\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 12275 Val: 1504 Test: 1548\n"
          ]
        }
      ],
      "source": [
        "# # Load d·ªØ li·ªáu g·ªëc\n",
        "# with open(\"/content/drive/MyDrive/NLP/qa_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "#     data = json.load(f)\n",
        "\n",
        "# Shuffle ƒë·ªÉ ph√¢n ph·ªëi ƒë·ªÅu (r·∫•t quan tr·ªçng cho QA)\n",
        "random.shuffle(cleaned_data)\n",
        "\n",
        "n = len(cleaned_data)\n",
        "train_data = cleaned_data[:int(0.8*n)]\n",
        "val_data   = cleaned_data[int(0.8*n):int(0.9*n)]\n",
        "test_data  = cleaned_data[int(0.9*n):]\n",
        "\n",
        "print(\"Train:\", len(train_data))\n",
        "print(\"Val:\", len(val_data))\n",
        "print(\"Test:\", len(test_data))\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
        "\n",
        "# D√πng dataset c≈© c·ªßa b·∫°n\n",
        "train_ds = QADataset(train_data, tokenizer, MAX_LEN)\n",
        "val_ds   = QADataset(val_data, tokenizer, MAX_LEN)\n",
        "test_ds  = QADataset(test_data, tokenizer, MAX_LEN)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "print(\"Train:\", len(train_ds), \"Val:\", len(val_ds), \"Test:\", len(test_ds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xJ2ufFeaTEB",
        "outputId": "ba9f7b13-32a4-4883-bbad-1f678cdc9bbe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======== DATASET ERROR REPORT ========\n",
            "L1. Answer not in context: 0\n",
            "L2. Tokenization mismatch: 24351\n",
            "L3. Truncated (answer removed): 0\n",
            "L4. Strange character errors: 0\n",
            "TOTAL SAMPLES: 24649\n",
            "VALID SAMPLES: 298\n",
            "======================================\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def normalize_text(s):\n",
        "    return re.sub(r\"\\s+\", \" \", s.strip())\n",
        "\n",
        "def check_dataset_errors(data, tokenizer, max_length=256):\n",
        "    errors_L1 = []   # answer not in context\n",
        "    errors_L2 = []   # tokenization mismatch\n",
        "    errors_L3 = []   # truncated removed answer\n",
        "    errors_L4 = []   # weird chars / format issues\n",
        "\n",
        "    for idx, ex in enumerate(data):\n",
        "        ctx = ex[\"context\"]\n",
        "        ques = ex[\"question\"]\n",
        "        ans = ex[\"answer\"]\n",
        "\n",
        "        # ============================\n",
        "        # L1: answer kh√¥ng n·∫±m trong context g·ªëc\n",
        "        # ============================\n",
        "        if ans not in ctx:\n",
        "            errors_L1.append(idx)\n",
        "            continue\n",
        "\n",
        "        # ============================\n",
        "        # L2: Check tokenize ‚Üí reconstruct mismatch\n",
        "        # ============================\n",
        "        ctx_tokens = tokenizer.tokenize(ctx)\n",
        "\n",
        "        reconstructed = \"\"\n",
        "        for tk in ctx_tokens:\n",
        "            if tk.startswith(\"‚ñÅ\"):\n",
        "                piece = tk[1:] if reconstructed == \"\" else \" \" + tk[1:]\n",
        "            else:\n",
        "                piece = tk\n",
        "            reconstructed += piece\n",
        "\n",
        "        if normalize_text(ans) not in normalize_text(reconstructed):\n",
        "            errors_L2.append(idx)\n",
        "            continue\n",
        "\n",
        "        # ============================\n",
        "        # L3: simulated truncate check\n",
        "        # encode pair and see if end_pos is out of range\n",
        "        # ============================\n",
        "        enc = tokenizer(\n",
        "            ques,\n",
        "            ctx,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_length,\n",
        "            return_tensors=None\n",
        "        )\n",
        "\n",
        "        q_tokens = tokenizer.tokenize(ques)\n",
        "        offset = 1 + len(q_tokens) + 1  # <s> Q </s>\n",
        "\n",
        "        # t√¨m l·∫°i v·ªã tr√≠ answer trong reconstructed\n",
        "        start_char = reconstructed.find(ans)\n",
        "        end_char = start_char + len(ans)\n",
        "\n",
        "        # map char ‚Üí reconstructed token index\n",
        "        start_tok = end_tok = None\n",
        "        curr = 0\n",
        "        token_offsets = []\n",
        "        for tk in ctx_tokens:\n",
        "            if tk.startswith(\"‚ñÅ\"):\n",
        "                piece = tk[1:] if curr == 0 else \" \" + tk[1:]\n",
        "            else:\n",
        "                piece = tk\n",
        "            s = curr\n",
        "            curr += len(piece)\n",
        "            e = curr\n",
        "            token_offsets.append((s, e))\n",
        "\n",
        "        for i,(s,e) in enumerate(token_offsets):\n",
        "            if s <= start_char < e:\n",
        "                start_tok = i\n",
        "            if s < end_char <= e:\n",
        "                end_tok = i\n",
        "\n",
        "        if start_tok is None or end_tok is None:\n",
        "            errors_L2.append(idx)\n",
        "            continue\n",
        "\n",
        "        start_pos = start_tok + offset\n",
        "        end_pos = end_tok + offset\n",
        "\n",
        "        if end_pos >= max_length:\n",
        "            errors_L3.append(idx)\n",
        "            continue\n",
        "\n",
        "        # ============================\n",
        "        # L4: detect weird characters\n",
        "        # ============================\n",
        "        if any(ord(c) > 60000 for c in ctx + ans):\n",
        "            errors_L4.append(idx)\n",
        "\n",
        "    return errors_L1, errors_L2, errors_L3, errors_L4\n",
        "\n",
        "\n",
        "# üîç CH·∫†Y CHECK L·ªñI\n",
        "L1, L2, L3, L4 = check_dataset_errors(cleaned_data, tokenizer, MAX_LEN)\n",
        "\n",
        "print(\"======== DATASET ERROR REPORT ========\")\n",
        "print(\"L1. Answer not in context:\", len(L1))\n",
        "print(\"L2. Tokenization mismatch:\", len(L2))\n",
        "print(\"L3. Truncated (answer removed):\", len(L3))\n",
        "print(\"L4. Strange character errors:\", len(L4))\n",
        "print(\"TOTAL SAMPLES:\", len(cleaned_data))\n",
        "print(\"VALID SAMPLES:\", len(cleaned_data) - (len(L1)+len(L2)+len(L3)+len(L4)))\n",
        "print(\"======================================\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "PPUL1eLhS24I"
      },
      "outputs": [],
      "source": [
        "class PhoBERTForQA(nn.Module):\n",
        "    def __init__(self, name=\"vinai/phobert-base\"):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(name)\n",
        "        H = self.encoder.config.hidden_size\n",
        "        self.qa_head = nn.Linear(H, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, start_positions=None, end_positions=None):\n",
        "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden = out.last_hidden_state\n",
        "\n",
        "        logits = self.qa_head(last_hidden)  # [B, L, 2]\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        loss = None\n",
        "        if start_positions is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(start_logits, start_positions) + loss_fn(end_logits, end_positions)\n",
        "\n",
        "        return {\"loss\": loss, \"start_logits\": start_logits, \"end_logits\": end_logits}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "25wVnbcqS52z"
      },
      "outputs": [],
      "source": [
        "class QATrainer:\n",
        "    def __init__(self, model, train_loader, val_loader, lr=1e-5, epochs=5, weight_decay=0.01):\n",
        "        self.model = model.to(DEVICE)\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.epochs = epochs\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = torch.optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=lr,\n",
        "            weight_decay=weight_decay\n",
        "        )\n",
        "\n",
        "        # Scheduler (warmup 10%)\n",
        "        warmup_steps = int(0.1 * len(train_loader) * epochs)\n",
        "        total_steps = len(train_loader) * epochs\n",
        "\n",
        "        self.scheduler = get_linear_schedule_with_warmup(\n",
        "            self.optimizer,\n",
        "            num_warmup_steps=warmup_steps,\n",
        "            num_training_steps=total_steps\n",
        "        )\n",
        "\n",
        "    # ============================\n",
        "    #        TRAIN EPOCH\n",
        "    # ============================\n",
        "    def train_epoch(self, epoch_idx):\n",
        "        self.model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        progress = tqdm(self.train_loader, desc=f\"Train Epoch {epoch_idx+1}/{self.epochs}\")\n",
        "\n",
        "        for batch in progress:\n",
        "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            out = self.model(**batch)\n",
        "            loss = out[\"loss\"]\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "\n",
        "            self.optimizer.step()\n",
        "            self.scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            avg_loss = total_loss / (progress.n + 1)\n",
        "\n",
        "            progress.set_postfix(loss=f\"{avg_loss:.4f}\")\n",
        "\n",
        "        return total_loss / len(self.train_loader)\n",
        "\n",
        "    # ============================\n",
        "    #          VALIDATION\n",
        "    # ============================\n",
        "    @torch.no_grad()\n",
        "    def val_epoch(self, epoch_idx):\n",
        "        self.model.eval()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        progress = tqdm(self.val_loader, desc=f\"Val Epoch {epoch_idx+1}/{self.epochs}\")\n",
        "\n",
        "        for batch in progress:\n",
        "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "            out = self.model(**batch)\n",
        "            total_loss += out[\"loss\"].item()\n",
        "\n",
        "        return total_loss / len(self.val_loader)\n",
        "\n",
        "    # ============================\n",
        "    #              FIT\n",
        "    # ============================\n",
        "    def fit(self):\n",
        "        best_val = float(\"inf\")\n",
        "        best_state = None\n",
        "\n",
        "        print(\"üöÄ B·∫Øt ƒë·∫ßu training PhoBERT QA...\")\n",
        "\n",
        "        for ep in range(self.epochs):\n",
        "            train_loss = self.train_epoch(ep)\n",
        "            val_loss = self.val_epoch(ep)\n",
        "\n",
        "            print(f\"\\nEpoch {ep+1}/{self.epochs} | Train={train_loss:.4f} | Val={val_loss:.4f}\")\n",
        "\n",
        "            if val_loss < best_val:\n",
        "                best_val = val_loss\n",
        "                best_state = {k: v.cpu().clone() for k, v in self.model.state_dict().items()}\n",
        "\n",
        "        # Load best checkpoint\n",
        "        if best_state is not None:\n",
        "            self.model.load_state_dict(best_state)\n",
        "            print(f\"\\nüî• Loaded best checkpoint (val_loss={best_val:.4f})\")\n",
        "\n",
        "        return self.model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0mc8jI6S9JW",
        "outputId": "45744f07-e2d4-4c48-ac9a-e54127ede1fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ B·∫Øt ƒë·∫ßu training PhoBERT QA...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 1/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1535/1535 [09:51<00:00,  2.59it/s, loss=5.8142]\n",
            "Val Epoch 1/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 188/188 [00:20<00:00,  9.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/5 | Train=5.8142 | Val=3.1910\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 2/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1535/1535 [09:50<00:00,  2.60it/s, loss=2.9965]\n",
            "Val Epoch 2/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 188/188 [00:19<00:00,  9.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2/5 | Train=2.9965 | Val=2.6745\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 3/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1535/1535 [09:50<00:00,  2.60it/s, loss=2.3883]\n",
            "Val Epoch 3/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 188/188 [00:19<00:00,  9.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3/5 | Train=2.3883 | Val=2.5735\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 4/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1535/1535 [09:50<00:00,  2.60it/s, loss=2.0367]\n",
            "Val Epoch 4/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 188/188 [00:19<00:00,  9.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4/5 | Train=2.0367 | Val=2.5920\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 5/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1535/1535 [09:49<00:00,  2.60it/s, loss=1.8379]\n",
            "Val Epoch 5/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 188/188 [00:19<00:00,  9.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5/5 | Train=1.8379 | Val=2.5970\n",
            "\n",
            "üî• Loaded best checkpoint (val_loss=2.5735)\n"
          ]
        }
      ],
      "source": [
        "model = PhoBERTForQA()\n",
        "trainer = QATrainer(model, train_loader, val_loader, lr=1e-5, epochs=5)\n",
        "\n",
        "model = trainer.fit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "sJzFWzMNTBWj"
      },
      "outputs": [],
      "source": [
        "class ExtractiveQAModel:\n",
        "    def __init__(self, model, tokenizer, max_length=256, top_k=8, max_answer_len=32):\n",
        "        self.model = model\n",
        "        self.tok = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.top_k = top_k\n",
        "        self.max_answer_len = max_answer_len\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict_span(self, ctx, ques):\n",
        "        self.model.eval()\n",
        "\n",
        "        enc = self.tok(\n",
        "            ques, ctx,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        enc.pop(\"token_type_ids\", None)\n",
        "        enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
        "\n",
        "        out = self.model(**enc)\n",
        "        s_log = out[\"start_logits\"][0]\n",
        "        e_log = out[\"end_logits\"][0]\n",
        "\n",
        "        s_top = torch.topk(s_log, self.top_k)\n",
        "        e_top = torch.topk(e_log, self.top_k)\n",
        "\n",
        "        best = (-1e10, 0, 0)\n",
        "\n",
        "        for i, s_idx in enumerate(s_top.indices):\n",
        "            for j, e_idx in enumerate(e_top.indices):\n",
        "                s = s_idx.item()\n",
        "                e = e_idx.item()\n",
        "\n",
        "                if e < s: continue\n",
        "                if (e - s + 1) > self.max_answer_len: continue\n",
        "\n",
        "                score = s_top.values[i] + e_top.values[j]\n",
        "                if score > best[0]:\n",
        "                    best = (score, s, e)\n",
        "\n",
        "        _, s, e = best\n",
        "        ids = enc[\"input_ids\"][0][s:e+1]\n",
        "        return self.tok.decode(ids, skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Q8bplz06TJ5n"
      },
      "outputs": [],
      "source": [
        "def normalize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def em(pred, gold):\n",
        "    return int(normalize(pred) == normalize(gold))\n",
        "\n",
        "def f1(pred, gold):\n",
        "    pt = normalize(pred).split()\n",
        "    gt = normalize(gold).split()\n",
        "    if len(pt) == 0 or len(gt) == 0: return 0\n",
        "    c = Counter(pt) & Counter(gt)\n",
        "    num_same = sum(c.values())\n",
        "    if num_same == 0: return 0\n",
        "    precision = num_same / len(pt)\n",
        "    recall = num_same / len(gt)\n",
        "    return 2 * precision * recall / (precision + recall)\n",
        "\n",
        "def evaluate_extractive(model, data):\n",
        "    EM, F1 = [], []\n",
        "    for ex in tqdm(data, desc=\"Eval\"):\n",
        "        pred = model.predict_span(ex[\"context\"], ex[\"question\"])\n",
        "        EM.append(em(pred, ex[\"answer\"]))\n",
        "        F1.append(f1(pred, ex[\"answer\"]))\n",
        "    return np.mean(EM), np.mean(F1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQ3wZQ6HTWvn",
        "outputId": "f28b0548-40f1-493e-fb12-3686626916be"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Eval:  29%|‚ñà‚ñà‚ñâ       | 710/2465 [00:13<00:32, 53.80it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Eval:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2082/2465 [00:40<00:07, 52.46it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2465/2465 [00:47<00:00, 51.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PHOBERT FINAL ‚Äî EM: 0.017849898580121704 F1: 0.551192840657221\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "extractive = ExtractiveQAModel(model, tokenizer)\n",
        "\n",
        "EM, F1 = evaluate_extractive(extractive, test_data)\n",
        "print(\"PHOBERT FINAL ‚Äî EM:\", EM, \"F1:\", F1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2D7Jf56Xz40",
        "outputId": "aee41564-29b7-458f-d01f-6c829654b176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Testing 5 random samples...\n",
            "\n",
            "===== SAMPLE 1 =====\n",
            "Q: ƒêi·ªÅu g√¨ c√≥ th·ªÉ x·∫£y ra n·∫øu kh√¥ng c√≥ s·ªë c·ªïng c·ªë ƒë·ªãnh ƒë∆∞·ª£c s·ª≠ d·ª•ng cho BOOTP?\n",
            "Pred: , c√≥ th·ªÉ m√°y kh√°ch kh√°c ƒëang l·∫Øng nghe tr√™n c√πng m·ªôt c·ªïng nh∆∞ng mong ƒë·ª£i ƒëi·ªÅu g√¨ ƒë√≥ kh√°@@\n",
            "Gold: N·∫øu kh√¥ng c√≥ s·ªë c·ªïng c·ªë ƒë·ªãnh ƒë∆∞·ª£c s·ª≠ d·ª•ng, c√≥ th·ªÉ m√°y kh√°ch kh√°c ƒëang l·∫Øng nghe tr√™n c√πng m·ªôt c·ªïng nh∆∞ng mong ƒë·ª£i ƒëi·ªÅu g√¨ ƒë√≥ kh√°c.\n",
            "\n",
            "===== SAMPLE 2 =====\n",
            "Q: T·ªïng n·ª£ c·ªßa Hoa K·ª≥ v√†o ng√†y 20 th√°ng 1 nƒÉm 2009 l√† bao nhi√™u?\n",
            "Pred: l√† 10,63 ngh√¨n t·ª∑ ƒë√¥\n",
            "Gold: 10,63 ngh√¨n t·ª∑ ƒë√¥ la.\n",
            "\n",
            "===== SAMPLE 3 =====\n",
            "Q: C√° heo v·∫°ch c√≤n ƒë∆∞·ª£c g·ªçi b·∫±ng nh·ªØng t√™n th√¥ng d·ª•ng n√†o kh√°c?\n",
            "Pred: C√° heo xanh ƒëu√¥i ƒë·ªè, c√° heo ƒëu√¥i ƒë·ªè, c√° heo v·∫°ch hay c√° nanh\n",
            "Gold: √° heo xanh ƒëu√¥i ƒë·ªè, c√° heo ƒëu√¥i ƒë·ªè, c√° \n",
            "\n",
            "===== SAMPLE 4 =====\n",
            "Q: Nh·ªØng tuy·∫øn ƒë∆∞·ªùng n√†o ƒëi qua huy·ªán H·∫° Lang?\n",
            "Pred: tr√™n t·ªânh l·ªô 207 v√† Qu·ªëc l·ªô 4@@\n",
            "Gold:  n·∫±m tr√™n t·ªânh l·ªô 207 v√† Qu·ªëc l·ªô 4A\n",
            "\n",
            "===== SAMPLE 5 =====\n",
            "Q: Gen sinh ung th∆∞ ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o?\n",
            "Pred: L√† c√°c y·∫øu t·ªë k√≠ch th√≠ch t·∫ø b√†o tƒÉng tr∆∞·ªü@@\n",
            "Gold:  c√≥ th·ªÉ l√† m·ªôt t√≠n hi·ªáu l√†m cho t·∫ø b√†o t·ªïng h·ª£p m·ªôt lo·∫°i th·ª• th·ªÉ n√†o ƒë√≥, ƒë·ªÉ tƒÉng nh·∫°y c·∫£m v√† tƒÉng ƒë√°p ·ª©ng v·ªõi nh·ªØng y·∫øu t·ªë l√†m t·∫ø b√†o ƒë·∫©y m·∫°nh ho·∫°t ƒë·ªông ph√¢n b√†o, t·ªïng h·ª£p DNA\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def test_random_samples(extractive_model, dataset, num_samples=5):\n",
        "    print(f\"\\nüîç Testing {num_samples} random samples...\\n\")\n",
        "    samples = random.sample(dataset, num_samples)\n",
        "\n",
        "    for i, ex in enumerate(samples):\n",
        "        ctx   = ex[\"context\"]\n",
        "        ques  = ex[\"question\"]\n",
        "        gold  = ex[\"answer\"]\n",
        "\n",
        "        pred = extractive_model.predict_span(ctx, ques)\n",
        "\n",
        "        print(f\"===== SAMPLE {i+1} =====\")\n",
        "        print(\"Context:\", ctx)\n",
        "        print(\"Q:\", ques)\n",
        "        print(\"Pred:\", pred)\n",
        "        print(\"Gold:\", gold)\n",
        "        print()\n",
        "test_random_samples(extractive, test_data, num_samples=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7r1Dtd2UTXzG",
        "outputId": "7a863f6b-0359-4236-ecbd-008336fc06ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved to /content/drive/MyDrive/NLP/phobert-qa-final\n"
          ]
        }
      ],
      "source": [
        "save_path = \"/content/drive/MyDrive/NLP/phobert-qa-final\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "torch.save(model.state_dict(), f\"{save_path}/pytorch_model.bin\")\n",
        "with open(f\"{save_path}/config.json\",\"w\") as f:\n",
        "    json.dump(model.encoder.config.to_dict(), f)\n",
        "\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(\"Saved to\", save_path)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
