import streamlit as st
import torch

# ==== IMPORT MODEL LOADER C·ª¶A B·∫†N ====
# copy l·∫°i 3 h√†m load: load_phobert_model, load_vit5_model, load_qwen_model
from model import load_phobert_model, load_vit5_model, load_qwen_model

device = "cuda" if torch.cuda.is_available() else "cpu"

# ============================
# üìå LOAD MODELS (lazy loading)
# ============================

@st.cache_resource
def load_models():
    return {
        "PhoBERT (Extractive)": load_phobert_model("D:\\NLP\\model\\phobert_qa_model_extend"),
        "ViT5 (Generative)":    load_vit5_model("D:\\NLP\\model\\vit5_qa_model_extend"),
        "Qwen 0.6B (Generative)": load_qwen_model("D:\\NLP\\model\\qwen3_qa_model_extend"),
    }

models = load_models()


# ============================
# üìå APP UI
# ============================
st.set_page_config(page_title="Vietnamese QA Demo", layout="wide")

st.title("üáªüá≥ Vietnamese QA Model Demo")
st.write("Demo th·ª≠ nghi·ªám m√¥ h√¨nh H·ªèi‚Äìƒê√°p: PhoBERT, ViT5, Qwen")


# ====== SIDEBAR ======
st.sidebar.header("‚öôÔ∏è Tu·ª≥ ch·ªçn")
model_name = st.sidebar.selectbox(
    "Ch·ªçn m√¥ h√¨nh",
    list(models.keys())
)

temperature = st.sidebar.slider("Nhi·ªát ƒë·ªô (d√πng cho model generative)", 0.0, 1.5, 0.3)


# ====== INPUT AREA ======
st.subheader("üìù Nh·∫≠p d·ªØ li·ªáu")

context = st.text_area("Ng·ªØ c·∫£nh:", height=200)
question = st.text_input("C√¢u h·ªèi:")

run = st.button("üöÄ Run Model")


# ============================
# üìå RUN MODEL
# ============================
if run:
    if not context.strip() or not question.strip():
        st.error("Vui l√≤ng nh·∫≠p ƒë·∫ßy ƒë·ªß Context v√† Question.")
    else:
        st.info(f"ƒêang ch·∫°y m√¥ h√¨nh **{model_name}**‚Ä¶")

        model_fn = models[model_name]

        try:
            # t·∫•t c·∫£ c√°c model b·∫°n ƒë√£ build ƒë·ªÅu tu√¢n theo signature:
            # model_fn([contexts], [questions]) -> [answers]
            if model_name == "PhoBERT (Extractive)":
                answer = model_fn.predict([context], [question])[0]
            else:
                answer = model_fn([context], [question])[0]


            st.success("‚ú® K·∫øt qu·∫£ tr·∫£ l·ªùi:")
            st.write(answer)

        except Exception as e:
            st.error(f"L·ªói khi ch·∫°y model: {str(e)}")

# ============================
# üìå FOOTER
# ============================
st.markdown("---")
st.caption("Demo QA ‚Ä¢ Built for your final project ‚Ä¢ Streamlit + Huggingface models")
