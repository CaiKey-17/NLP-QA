import os
import json
import time
import multiprocessing as mp
from tqdm import tqdm
from google import genai
from google.genai.errors import APIError


# ==============================
# CONFIG
# ==============================
API_KEYS = [
    "KEY_1",
    "KEY_2",
    "KEY_3",
    "KEY_4",
    "KEY_5",
]

INPUT_PATH = r"D:\NLP\QA_NLP\data\train.jsonl"
OUTPUT_DIR = r"D:\NLP\QA_NLP\data\synthetic_parts"
FINAL_OUTPUT = r"D:\NLP\QA_NLP\data\synthetic_train.jsonl"

MODEL_NAME = "gemini-2.5-flash"
MAX_RETRIES = 5
BASE_WAIT = 5

os.makedirs(OUTPUT_DIR, exist_ok=True)


# ==============================
# LOAD JSONL
# ==============================
def load_jsonl(path):
    data = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data


# ==============================
# PROMPT
# ==============================
def make_prompt(context, question, answer):
    return f"""
B·∫°n l√† h·ªá th·ªëng t·∫°o d·ªØ li·ªáu QA ti·∫øng Vi·ªát.

D·ª±a v√†o th√¥ng tin ban ƒë·∫ßu:
- Context: {context}
- C√¢u h·ªèi g·ªëc: {question}
- C√¢u tr·∫£ l·ªùi g·ªëc: {answer}

Nhi·ªám v·ª•:
- H√£y sinh th√™m **3 c·∫∑p c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi m·ªõi**.
- C√¢u h·ªèi ph·∫£i t·ª± nhi√™n, kh√¥ng nh·∫Øc ƒë·∫øn ‚Äúƒëo·∫°n vƒÉn tr√™n‚Äù.
- ƒê√°p √°n ph·∫£i ƒë√∫ng d·ª±a tr√™n context.
- **KH√îNG sinh label n·ªØa**.
- Output **PH·∫¢I** l√† JSON d·∫°ng list:
[
  {{"question":"...", "answer":"..."}},
  {{"question":"...", "answer":"..."}},
  {{"question":"...", "answer":"..."}}
]
"""


def extract_json(text):
    try:
        s = text.index("[")
        e = text.rindex("]") + 1
        return json.loads(text[s:e])
    except:
        return None


# ==============================
# WORKER
# ==============================
def worker(api_key, chunk, out_file):
    os.environ["GEMINI_API_KEY"] = api_key
    client = genai.Client()

    results = []

    pbar = tqdm(chunk, desc=f"KEY {api_key[-6:]}", position=mp.current_process()._identity[0])

    for item in pbar:
        context = item.get("context", "")
        question = item.get("question", "")
        answer = item.get("answer", "")

        prompt = make_prompt(context, question, answer)

        for retry in range(MAX_RETRIES):
            try:
                resp = client.models.generate_content(
                    model=MODEL_NAME,
                    contents=[prompt],
                    config={
                        "temperature": 0.7,
                        "response_mime_type": "application/json"
                    }
                )

                txt = resp.text.strip()
                qa_list = extract_json(txt)

                if qa_list:
                    for qa in qa_list:
                        results.append({
                            "context": context,
                            "question": qa["question"],
                            "answer": qa["answer"]
                        })
                break

            except Exception as e:
                wait = BASE_WAIT * (2 ** retry)
                print(f"‚ö† KEY {api_key[-6:]} l·ªói, retry sau {wait}s")
                time.sleep(wait)

    # save
    with open(out_file, "w", encoding="utf-8") as f:
        for item in results:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

    print(f"üéâ KEY {api_key[-6:]} ho√†n t·∫•t ‚Üí {out_file}")


# ==============================
# MAIN
# ==============================
if __name__ == "__main__":
    print("üì• ƒêang load train.jsonl ...")
    data = load_jsonl(INPUT_PATH)
    total = len(data)
    print(f"üìå T·ªïng s·ªë m·∫´u train: {total}")

    # chia theo s·ªë API key
    n = len(API_KEYS)
    chunk_size = total // n

    chunks = [data[i*chunk_size:(i+1)*chunk_size] for i in range(n)]
    # ph·∫ßn d∆∞
    if len(chunks) < n:
        chunks[-1].extend(data[n*chunk_size:])

    print(f"üîÄ ƒê√£ chia th√†nh {len(chunks)} ph·∫ßn")

    processes = []
    out_files = []

    for idx, api_key in enumerate(API_KEYS):
        part = chunks[idx]
        out_file = os.path.join(OUTPUT_DIR, f"synthetic_part_{idx+1}.jsonl")
        out_files.append(out_file)

        p = mp.Process(target=worker, args=(api_key, part, out_file))
        p.start()
        processes.append(p)

    for p in processes:
        p.join()

    # merge output
    print("üì¶ Gh√©p c√°c ph·∫ßn synthetic l·∫°i...")
    with open(FINAL_OUTPUT, "w", encoding="utf-8") as fout:
        for path in out_files:
            if os.path.exists(path):
                with open(path, "r", encoding="utf-8") as f:
                    for line in f:
                        fout.write(line)

    print("üéâ HO√ÄN T·∫§T!")
    print(f"üìÅ Synthetic train l∆∞u t·∫°i: {FINAL_OUTPUT}")
